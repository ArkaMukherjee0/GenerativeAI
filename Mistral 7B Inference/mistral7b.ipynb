{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Device:\", device)\n",
    "if device == 'cuda':\n",
    "  print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c867ce9ba7494752bdf893a03b8a76bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharding:** Sharding is the process of loading a larger model in smaller increments (approximately 1.9 GB each) in the notebook and subsequently aggregated into a single file. This allows us to seamlessly utilize the LLM on a GPU with lesser memory than its total size.\n",
    "\n",
    "We use sharding to load the 14 GB Mistral 7B model onto the 8GB RTX 4060 GPU by loading it in small packets of 1.9GB each. We are loading a 4-bit quantized version of the model, which effectively compresses 7B parameters to ~14 GB.\n",
    "\n",
    "Other dependencies used:\n",
    "1. BitsAndBytesConfig file:\n",
    "    - `load_in_4bit`: set to `True` to allow loading the 4-bit quantized version of the model\n",
    "    - `bnb_4bit_use_double_quant`: set to `True` to allow additional quantization after `load_in_4bit` to save an additional 0.4 bits per parameter\n",
    "    - `bnb_4bit_quant_type`: set to `nf4` to allow operations on 4-bit data types\n",
    "    - `bnb_4bit_compute_dtype`: set to bfloat16 to reduce memory overhead during inference\n",
    "2. AutoModelForCausalLM.from_pretrained:\n",
    "    - loads a pre-trained model given the model path and the BitsAndBytes config file. We are loading Mistral 7B here.\n",
    "3. AutoTokenizer.from_pretrained:\n",
    "    - loads the pre-trained tokenizer for Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15c34a5343541ce88eef208a719b58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bf9f404c6e4417a97cc26519b27f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CoolA\\Code\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\CoolA\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2600ba5b31e4e299c28b63c47172d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211ad56ee06745f4995f0937e4fae9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada26e96e173480598f3ab848b259d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading Mistral 7B\n",
    "original_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
    "bnb_config = BitsAndBytesConfig (\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for running LLMs\n",
    "1. HuggingFace Transformers pipeline\n",
    "    - Simple API to load all the complex code, including those for Natural Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction, and Question-Answering.\n",
    "    - Simple inference is a `text-generation` task.\n",
    "2. LangChain HuggingFace pipeline\n",
    "    - Integrates `transformers.pipeline` with LangChain\n",
    "    - LangChain is used extensively with LLMs. Hence, it is integrated from the get go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.5,\n",
    "    do_sample=True,\n",
    ")\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CoolA\\Code\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of life (answer in 100 words)? The purpose of life is to live a fulfilling existence that brings happiness, peace, and satisfaction. This can be achieved through various means such as pursuing one's passions, developing meaningful relationships, contributing to society, and striving for personal growth and self-improvement. Ultimately, the purpose of life is to find joy and contentment in the present moment, while also striving to make a positive impact on the world around us.\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the purpose of life (answer in 100 words)?\"\n",
    "response = mistral_llm.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a reliable and trustworthy assistant, providing helpful and respectful responses that preceisely address the context.\n",
      "Answer the question below with the given context:\n",
      "\n",
      "Virat Kohli was born on November 5, 1988, in Delhi, India. He grew up in Delhi and was one of the first to train at the West Delhi Cricket Academy, created in 1998. In 2002 he played for the Delhi Under-15 team and was the highest run scorer in the 2003–04 Vijay Merchant Trophy, playing for the Delhi Under-17 team.\n",
      "\n",
      "In February 2006 Kohli made his domestic debut for Delhi in a one-day match against Services (a team representing the Indian armed forces) but did not get a chance to bat. He scored only 10 runs in his first-class debut (first-class cricket refers to matches that last three or more days and feature two sides of 11 players each) against Tamil Nadu in November that year. He scored 90 runs in difficult conditions in a first-class match against Karnataka in December, helping Delhi draw the Test. In April 2007 he scored 35 runs in his T20 domestic debut against Himachal Pradesh.\n",
      "\n",
      "Kohli captained the Under-19 Indian cricket team to victory at the ICC Under-19 World Cup in Kuala Lumpur, Malaysia, in 2008. His exploits were rewarded with an IPL contract from RCB for $30,000. He also made his international debut in an ODI that year, opening the batting and scoring 12 in a defeat of Sri Lanka in Dambulla, Sri Lanka. In 2009 he scored 405 runs in nine innings in the Emerging Players Tournament in Australia, ensuring that he would be at the top of the national team selectors’ minds.\n",
      "\n",
      "When did Virat Kohli first win?\n",
      "Answer:\n",
      "Virat Kohli won his first title as captain of the Under-19 Indian cricket team at the ICC Under-19 World Cup in Kuala Lumpur, Malaysia, in 2008.\n",
      "\n",
      "Virat Kohli won his first title as captain of the Under-19 Indian cricket team at the ICC Under-19 World Cup in Kuala Lumpur, Malaysia, in 2008.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = '''You are a reliable and trustworthy assistant, providing helpful and respectful responses that preceisely address the context.\n",
    "Answer the question below with the given context:\n",
    "{context}\n",
    "{question}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "question = 'When did Virat Kohli first win?'\n",
    "context = '''\n",
    "Virat Kohli was born on November 5, 1988, in Delhi, India. He grew up in Delhi and was one of the first to train at the West Delhi Cricket Academy, created in 1998. In 2002 he played for the Delhi Under-15 team and was the highest run scorer in the 2003–04 Vijay Merchant Trophy, playing for the Delhi Under-17 team.\n",
    "\n",
    "In February 2006 Kohli made his domestic debut for Delhi in a one-day match against Services (a team representing the Indian armed forces) but did not get a chance to bat. He scored only 10 runs in his first-class debut (first-class cricket refers to matches that last three or more days and feature two sides of 11 players each) against Tamil Nadu in November that year. He scored 90 runs in difficult conditions in a first-class match against Karnataka in December, helping Delhi draw the Test. In April 2007 he scored 35 runs in his T20 domestic debut against Himachal Pradesh.\n",
    "\n",
    "Kohli captained the Under-19 Indian cricket team to victory at the ICC Under-19 World Cup in Kuala Lumpur, Malaysia, in 2008. His exploits were rewarded with an IPL contract from RCB for $30,000. He also made his international debut in an ODI that year, opening the batting and scoring 12 in a defeat of Sri Lanka in Dambulla, Sri Lanka. In 2009 he scored 405 runs in nine innings in the Emerging Players Tournament in Australia, ensuring that he would be at the top of the national team selectors’ minds.\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['question', 'context'])\n",
    "\n",
    "llm_chain = prompt | mistral_llm | output_parser\n",
    "\n",
    "response = llm_chain.invoke({\"question\":question, \"context\":context})\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "# Applying regex to extract only answer from the LLM response\n",
    "match = re.search(r'Answer:\\n(.*?)$', response, re.DOTALL | re.MULTILINE)\n",
    "if match:\n",
    "    answer = match.group(1).strip()\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
